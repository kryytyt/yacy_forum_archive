Ich habe ja bei meinem IoT-Vortrag vor einer Weile \...
=======================================================

Date: 2017-08-20 03:32:56

Ich habe ja bei [meinem IoT-Vortrag](http://blog.fefe.de/?ts=a7eba6fb)
vor einer Weile am Ende ein paar Bonusfolien gehabt. Ich denke mir bei
Vorträgen immer, OK, die bezahlen mich dafür, dass ich ihnen sage, was
sie hören wollen (vielleicht nicht unbedingt von den Schlussfolgerungen,
aber vom Thema her), aber ich tue ihnen einen viel größeren Gefallen,
wenn ich ihnen auch sage, was sie hören *sollten*, nicht nur was sie
hören *wollen*.

Jedenfalls waren meine Bonusfolien u.a. [zu Machine
Learning](https://ptrace.fefe.de/iot/iot.html#88), und ich mache da den
Punkt, dass man gar nicht versteht, was man eigentlich gerade genau
trainiert hat. Ich brachte als Beispiel Adversarial Examples, aber das
lag vor allem daran, dass ich kein besseres Beispiel hatte. Das besser
Beispiel ist jetzt da: [Vice über das Anti-Bullying-AI von
Google](https://motherboard.vice.com/en_us/article/qvvv3p/googles-anti-bullying-ai-mistakes-civility-for-decency).
Die Primärquelle [ist dieser Artikel vom Februar
2017](https://qz.com/918640/alphabets-hate-fighting-ai-doesnt-understand-hate-yet/).
Money Quote aus dem:

> However, computer scientists and others on the internet have found the
> system unable to identify a wide swath of hateful comments, while
> categorizing innocuous word combinations like "hate is bad" and
> "garbage truck" as overwhelmingly toxic.

Ich möchte mal die steile These wagen, dass das daran liegt, dass
Menschen das Problem für zu schwierig hielten, um es selber lösen zu
können, und eine Abkürzung gesucht haben, bei denen am Ende eine KI
Schuld wäre, nicht sie. Bei der es am Ende eine (schlechte) Ausrede
gibt, wieso das nicht *mein* Totalversagen ist, was wir hier gerade
beobachten, sondern da hat halt, äh, like, voll die Technik versagt, und
so. AI is difficult, let\'s go shopping!

Die Einsicht daran bringt aber der Motherboard-Artikel gut auf den
Punkt. Was die ihrem Machine Learning antrainiert haben, weil es nämlich
einfacher zu trainieren ist als tatsächlich böse Aussagen, sind
Schimpfwörter. Money Quote:

> The tool seems to rank profanity as highly toxic, while deeply harmful
> statements are often deemed safe

Böse Wörter benutzen, das kann die KI (auch nicht wirklich, aber so
ansatzweise wenigstens) erkennen. Aber wenn jemand etwas wirklich böses
sagt, etwas, das den Gegenüber möglicherweise gar in den Selbstmord
treibt, so fies und gemein ist es, und es nagt die ganze Zeit am
Unterbewusstsein und du kriegst es nicht weg, sowas kann die KI nicht
erkennen.

Ich möchte an dieser Stelle zu Protokoll geben, dass ich auch nicht
möchte, dass eine KI sowas zu erkennen versucht. Denn das menschliche
Verhalten in Situationen, wo Software Regeln durchsetzen soll, ist immer
dasselbe. Das wird als Herausforderung genommen, nicht als hilfreiche
Polizeiarbeit. Das ist genau so \"after the
fact\"-Symptom-Filter-Schlangenöl wie Antiviren.

**Man kann soziale Probleme nicht technisch lösen**.
