YaCy Coding & Architecture â€¢ Re: Ranking Transparency Roadmap
=============================================================

Date: 2016-10-14 07:32:28

Hi reger and luc,\
\
UX is not really my specialty, but my guess is that to really define
\"understandable\" we would have to have some idea of what a user\'s
goal is.\
\
Some examples:\
\
1. \"I think YaCy is ranking a website too high or low by accident, I
want to figure out how this can be fixed by changing the ranking
rules.\"\
2. \"I think YaCy is ranking a website too high because that website is
using abusive SEO / spam techniques, I want to figure out what the
website is doing so we can make YaCy penalize such sites.\"\
3. \"I have a new idea for a YaCy ranking method and I want to figure
out whether it would be beneficial, and which pages would be most
affected.\"\
4. \"I have a website and I want to figure out how to change the site to
make it rank more highly in YaCy using the default YaCy settings.\"\
\
There may be some information that is beneficial for some of these use
cases, but is superfluous for others of these use cases. It might be
useful to consider these use cases independently for the purpose of
figuring out what information should be highlighted and how it should be
visualized. I think a good first step is to simply make the raw data
available, since this allows people to experiment with layers on top of
it, but I fully agree that making raw data available is not really
sufficient by itself for most real-world use cases. (Although for my
particular use cases, it\'s sufficient given that I\'m willing to code
some Python scripts to do my additional analysis.)\
\
In terms of optimization/learning, a common technique in machine
learning is backpropagation. Basically, this uses the partial derivative
of an output variable with respect to some input variable, to determine
how to change the input variable in order to optimize the output
variable. I\'m playing around with this technique in the context of YaCy
ranking, but I don\'t have any results to share yet. The important
takeaway here is that because backpropagation needs partial derivatives,
it needs to know what calculations were used to get the final ranking
score.\
\
One potentially useful way to get data for deciding how to optimize
ranking would be to use clickthrough data. There\'s not much of a
privacy implication to collecting clickthrough data as long as it\'s not
shared with peers, but my guess is that multiple nodes\' clickthrough
data would need to be combined in some way to get sufficiently
noise-free data. There are some ways that this could be done; I\'m
investigating using a social graph method where users\' own nodes do
optimization using their own clickthrough data but share a weighted sum
of their own optimizations and their friends\' optimizations. This is
reasonably private (users effectively act as a blind for the users in
their social graph), and reasonably Sybil-resistant (social graphs have
been reasonably well-studied for Sybil-resistance, including in the
context of Freenet, for which the stakes are a lot higher). I don\'t
have any practical results to share on that yet.\
\
Cheers!\
-Jeremy

Statistik: Verfasst von
[biolizard89](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=8861)
--- Fr Okt 14, 2016 6:32 am

------------------------------------------------------------------------
