Fragen und Antworten • erneutes crawlen
=======================================

Date: 2012-12-02 14:44:55

Hallo\
\
ich habe gesehen, dass sich einige Funktionen ja super entwickelt haben,
wie z.B. die Übersicht aller Domains (\"Host Browser\" - wer hat den
gemacht??) und wieviele Urls dahinter stecken oder auch die Top 1000
Domain Funktion.\
\
Ich hatte bei alexia.com versucht zu crawlen, aber das geht nicht wegen
robots-exclusion. Dann hatte ich das dort angebotene URL-Zip
heruntergeladen, mit den 1 MIO meistbesuchten Webseiten, die Zahlen
ersetzt und die Urls als <http://www>. (statt Kommata) als text im HTML
oder in einer XML gespeichert aus Excel.\
\
Leider kann mein HTML Editor die 100 MB Webseite nicht laden und ich
glaube yacy auch nicht.\
\
Dabei wäre es doch gut, diese URLs zu implementieren:
<http://s3.amazonaws.com/alexa-static/top-1m.csv.zip>\
\
Der Host Browser verlinkt als Funktion zu yacy, um die nächste Ebene zu
haben. Einige Domains kenne ich nicht und wollte die mal browsen aus der
yacy Seite heraus, kann man nicht einen Button machen, so dass man auf
den klickt und sich copy paste der URL sparen kann und dann die Domain
im Browser ansehen kann?\
\
Nebeneffekt: Dann kann man diese Seite auch nochmal abspeichern als html
und alle Hosts in der Seite Hostbrowser nochmal als Crawlstartseite
starten!!\
\
\
So dieser Hintergrund erzeugt folgende Frage:\
\
Ich habe nun eine Alexa Url, die in den Top 1000 Alexa Urls drin ist, in
yacy aber nicht. Ich crawle die nun.\
Wie kann dann sichergestellt sein, dass auch im Host Browser diese
Domain ebenso (mit den vielen Unterseiten) erstens ebenso vorhanden ist
oder gar ein entsprechenes Ranking aufgrund der Menge der Webseiten hat?
d.h. die Unterseiten alle auch gecrawled werden.\
\
Meine Lösung dazu wäre die Anregung, dass man per DHT eingehende URLs
bzw. Domains oder die Webseite des Domain-Host-Browser Ansicht
\*\*\*\*\*SELBST \*\*\*\*\*\* nochmal crawled oder es in regelmässigen
Abständen tut.\
\
D.h. wenn eine Domain-liste vorhanden ist, habe ich z.B. an meiner
gesehen, dass das russische Unix Forum ganz oben ist, und dass irgend
einer seinen Blog für Filme gut crawled. Damit wird aber der Index zu
einem Spako-getriebenen Index und ist nicht repräsentativ.\
\
\
Mein Vorschlag oder Frage ist daher, URLs können über DHT oder einen
eigenen oder remote Crawl kommen, ich fände es aber gut, wenn es
VIERTENS einen AUTO Crawl der Domains gäbe, so dass ich selbst anfage,
bei dem EIngang einer neuen, meinem Node bislang unbekannten Domain,
diese nochmal selbst zu crawlen. Auch wenn mir einer die Website oder
Webseiten oder die Domain des Unixforum sendet. Die Inhalte könnten ja
neu sei und es macht doch dann Sinn, es nochmal selbst als Startseite zu
crawlen.\
\
Wäre es möglich, eine default-on check box zu haben, dass die Webseite
einer dem Node neu bekannt werdenden Domain mit 1 Hop nochmal crawled?\
\
Insofern angenommen UnixForum wäre nicht top, sondern gar nicht bekannt,
dann würde hierzu ein Crawl gestartet und zwar nicht nur von mir,
sondern von jedem Node, der diese neue Domain bekommt per DHT.\
\
Es ist sehr interessant, die Top 1 Mio Domains von Alexa zu vergleichen
mit den von Yacy aus dem Host Browser.\
\
Wie bekommt man die Alexa Domains in Yacy ähnlich repräsentativ hinein?
Ich weiss, dass man die Zahl der gecrawlten Unterseiten und überhaupt
vorhandene Unterseiten als Rankingtreiber vergleicht mit den
Klickbasierten Rankings von Alexa.\
\
Aber dennoch sollten grosse Webseiten die bei Alexa sind auch durch yacy
gecrawlt werden.\
\
Hat das schon jemand versucht? und kann es ein Re-Crawl der Host Browser
Seite geben, z.B. indem man die Domain auch mit HTTP verlinkt in einem
zweiten Hyperlink dahinter?

Statistik: Verfasst von
[ribbon](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=193)
--- So Dez 02, 2012 2:44 pm

------------------------------------------------------------------------
