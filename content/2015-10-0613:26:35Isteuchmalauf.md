Ist euch mal aufgefallen, dass mein Blog relativ schnell \...
=============================================================

Date: 2015-10-06 13:26:35

Ist euch mal aufgefallen, dass mein Blog relativ schnell lädt?
Wahrscheinlich.

Habt ihr euch auch mal gefragt, warum dass so ist? Ist meine Anbindung
so toll? Meine Hardware? Meine Software? Cache ich so krass?

Nein. Ich cache hier nichts. Jede Anfrage, die reinkommt, erzeugt und
beendet zwei Prozesse und eine Kette aus 4 Interprozess-Kommunikationen.
Es gibt kein memcache, keinen Varnish, kein Load-Balancer, kein
Cassandra, kein Akamai, nichts. Der einzige Grund, wieso das bei mir
flutscht, ist weil ich die Anzahl der HTTP-Anfragen minimiert habe und
nichts externes reinlade. Bei mir kommt es im Allgemeinen bei einem
Zugriff zu drei HTTP-Requests. Einmal das HTML selbst, einmal das
favicon, und einmal das CSS, wenn ihr eines einbindet. favicon und CSS
sind statisch und müssen normalerweise nicht neu übertragen werden.
Bleibt als einziger Inhalt das HTML. Das komprimiere ich vor der
Übertragung, und benutze auch kein CMS, was das irgendwie aufblähen
könnte. Hier gibt es kein Typekit, kein React, kein jquery, keine
Facebook-Buttons, etc.

Das mag jetzt trivial erscheinen, aber überlegt euch mal folgendes.
Nehmen wir mal an, eine Webseite besteht aus 100 Elementen. Der
Webserver bei einem davon hat einen Schluckauf und braucht 10 Sekunden.
Dann lädt die Seite gefühlt langsam. Mal ganz abgesehen von der riesigen
Datenmenge, versteht sich, die durch so viele Elemente entstehen.
Alleine die Header sind ja schon 2 KB pro Anfrage.

Mein Webserver hat bestimmt auch gelegentlich mal einen schlechten
Request, der aus irgendeinem Grund langsam ist. Wenn bei mir jeder 1000.
Request gammelig ist, betrifft das nicht mal jeden hundersten User. Wenn
bei einer Site mit 100 Elementen jeder 1000. Request gammelig ist,
betrifft das jeden Zehnten.

Ich erwähne das alles bloß als Einleitung für eine Video-Empfehlung:
[Ein Talk darüber, wie man Latenz
misst](https://www.youtube.com/watch?v=lJ8ydIuPFeU). Es geht um den
Web-Kontext. Die Kurzzusammenfassung ist: Fast alle machen es falsch.
Ihr wahrscheinlich auch. Außer ihr habt eure Messtools selber gehackt,
und selbst dann möglicherweise immer noch.

Ich habe übrigens in gatling damals das Logformat extra so gemacht, dass
das eine angesprochene Problem nicht deshalb nicht messbar ist, weil die
vorliegenden Daten es nicht zulassen. Vielleicht sollte ich meine
Ad-Hoc-Messtools mal ein bisschen professionalisieren und
veröffentlichen, wenn die Tool-Situation wirklich so übel ist, wie er in
dem Vortrag sagt.
