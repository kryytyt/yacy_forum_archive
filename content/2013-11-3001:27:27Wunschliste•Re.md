Wunschliste • Re: Alte Links/ Einträge / Blacklist
==================================================

Date: 2013-11-30 01:27:27

man kann nur überprüfen ob Webseiten noch existieren indem man alle
links, die man kennt, wieder versucht zu laden. Das ist zwar
vorstellbar, aber strategisch nicht unbedingt sinnvoll. Beispielsweise
gehen Nutzer so vor, um wöchentlich alle Webseiten neu zu erfassen und
abei alles, was älter als zwei wochen ist und nicht mehr existiert zu
löschen:\
- bestimmte Webseiten sollen regelmäßig neu gecrawlt werden\
- damit das nicht so aufwendig ist, wird dabei im Crawl Start inden
\"Double-Check Rules\" das Flag \"Re-load\" aktiviert und auf 7 Tage
gestellt.\
- damit alles was älter als 14 Tage ist und nicht mehr existiert, wird
im Feld \"Clean-Up before Crawl Start\" das Flag \"Delete only old\"
aktiviert und auf 14 Tage gestellt.\
\
Dabei ist zu vermerken, dass ein Überprüfen von allen Links, wie du
vorschlägst nicht so effizient ist, wie ein Neuladen von allen Links,
die nicht im Cache sind und beim Löschen von älteren Dateien übrig
geblieben sind.\
\
Das ganze funktioniert so natürlich nur innerhalb der Zielsetzung,
einzelne Domänen zu crawlen. Wenn du den gesamten Index ohne Crawlerziel
\'auffrischen\' willst, dann muss man das tatsächlich so machen wie du
vorschlägst.

Statistik: Verfasst von
[Orbiter](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=2)
--- Sa Nov 30, 2013 1:27 am

------------------------------------------------------------------------
