Hilfe für Einsteiger und Anwender • Re: Crawl auf Startseiten beschränken
=========================================================================

Date: 2013-01-23 16:11:18

Ich habe nun folgende Abfrage
/solr/select?q=\*:\*&start=0&rows=100&fl=sku gemacht (\"robots.txt\"
beim Must-Not-Match Filter wurde nicht angegeben), bzw. habe ich dann
die rows gleich mal auf 2000 eingestellt und gleich alle Ergebnisse
angesehen und dann gleich auch mal den httpstatus\_i mit abgefragt (also
/solr/select?q=\*:\*&start=0&rows=2000&fl=sku,httpstatus\_i).\
\
Es sind sehr viele 404er im Ergebnis, alles robots.txt-Pages sowie viele
-1er (heißt das: keine Serverantwort?). Und dann gibt es noch
vereinzelte 301er, 403er, 503er, etc. D.h., die 376 zusätzlichen Urls im
Index sind scheinbar nicht gefundene robots.txt. Um diese Vermutung zu
bestätigen habe ich jetzt noch folgende Abfrage gemacht:
/solr/select?q=httpstatus\_i:404%20AND%20sku:\*robots\*&start=0&rows=2000&fl=httpstatus\_i,sku.
Und es sind tatsächlich 377 Treffer (wo der eine zusätzliche Treffer
herkommt hab ich jetzt auf die schnelle nicht rausbekommen
![;)](http://forum.yacy-websuche.de/images/smilies/icon_e_wink.gif "Wink")
).\
\
Dass diese nicht gefundenen robots.txt-Seiten in den Index gelangen
verwirrt mich zwar etwas, aber nun kenn ich mich aus. Ich werde wie zu
Beginn vorgeschlagen beim Must-Not-Match Filter die robots.txt
ausschließen und dann sollte alles passen.
![:)](http://forum.yacy-websuche.de/images/smilies/icon_e_smile.gif "Smile")

Statistik: Verfasst von
[hotel24](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=8871)
--- Mi Jan 23, 2013 4:11 pm

------------------------------------------------------------------------
