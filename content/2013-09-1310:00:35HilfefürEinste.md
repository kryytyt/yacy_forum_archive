Hilfe für Einsteiger und Anwender • Strategie für (externe) Foren-Crawls
========================================================================

Date: 2013-09-13 10:00:35

Hi,\
ich habe die letzten Tage unter anderem Foren in meinem Crawler gehabt.
Mittlerweile habe ich einen Stapel Regulärer Ausdrücke, um die
gefundenen Seiten auf die entsprechend Inhaltstragenden Seiten zu
reduzieren. Das ganze funktioniert einigermaßen brauchbar
![:D](http://forum.yacy-websuche.de/images/smilies/icon_e_biggrin.gif "Very Happy").
Mein ursprünglicher Gedanke war, anschließend per RSS Feed Import nur
noch Updates der entsprechenden Foren zu crawlen.\
\
So weit der Gedanke
![:)](http://forum.yacy-websuche.de/images/smilies/icon_e_smile.gif "Smile")
.. jedoch musste ich feststellen das die URLs im RSS Feed in der
Blacklist hängen bleiben, da Links in der Form

Code: 
:   `../viewtopic.php...&goto=newpost`

erzeugt werden. Diese werden aufgrund der Parameter (die zu Duplikaten
führen würden) geblockt.\
\
Das ganze wirft bei mir zwei Fragen auf..\

-   Hat jemand bereits eine brauchbare Strategie gefunden, um Foren so
    zu crawlen, das möglichst wenig Overhead entsteht?
-   Da ich aus dem Bereich der Informationswissenschaft komme, frage ich
    mich, ob man Yacy eine art \"Content Awareness\" verpassen könnte?
    BTW bin ich bei der Recherche auf das Research-Projekt
    [iRobot]{style="font-style: italic"} von Microsoft gestossen, die
    ähnliches versucht haben.

Statistik: Verfasst von
[surfvive](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=8791)
--- Fr Sep 13, 2013 9:00 am

------------------------------------------------------------------------
