Hilfe für Einsteiger und Anwender • Re: Crawl auf Startseiten beschränken
=========================================================================

Date: 2013-01-23 15:18:36

> <div>
>
> hotel24 hat geschrieben:\
> Die robots.txt wird scheinbar standardmäßig bei jeder Url aufgerufen
> und wird diese dann, wenn vorhanden, ebenfalls indiziert? Denn bei
> einem Test mit 1000 Urls waren letztendlich 1.376 Urls im Index, trotz
> Crawling Depth 0.\
>
> </div>

\
\
Die robots.txt wird nicht bei jeder URL sondern bei jedem neuen Host
geladen. Wenn deine Liste nur verschiede Hosts hat, dann wohl hier bei
jeder URL.\
Sie wird aber nicht indexiert! Die robots.txt landet in einer anderen
Datenbank.\
\
Mehr Seiten im Index kann sein, denn es werden auch Fehlerseiten und
Weiterleitungen rein geschrieben. Diese werden aber bei einer Suche
ausgeblendet. Es ist aber auffällig, dass hier so viele extradokumente
sind, da könnte noch irgendwo was faul sein. Kannst du rausbekommen was
im Index ist und da nicht sein soll?\
Rufe mal folgendes auf:\
/solr/select?q=\*:\*&start=0&rows=100&fl=sku\
da hast du eine Auswahl von 100 URLs aus deinem Index. Kannst du da
Auffälligkeiten finden?

Statistik: Verfasst von
[Orbiter](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=2)
--- Mi Jan 23, 2013 3:18 pm

------------------------------------------------------------------------
