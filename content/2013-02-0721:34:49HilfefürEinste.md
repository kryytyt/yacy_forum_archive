Hilfe für Einsteiger und Anwender • Re: Crawl auf Startseiten beschränken
=========================================================================

Date: 2013-02-07 21:34:49

> <div>
>
> hotel24 hat geschrieben:\
> In weiterer Folge möchte ich diese Seiten nun nur alle 7 Tage auf
> Änderungen überprüfen. Dazu habe ich im Bereich \"Document
> Double-Check\" die Checkbox \"Re-load\" mit 7 Tage markiert. Das
> Problem ist allerdings, dass diese Regel nicht für die Start-URLs
> angewendet wird. D.h. in meinem Falle werden trotz dieser Einstellung
> alle URLs jedes Mal frisch indiziert.\
>
> </div>

\
diese Beobachtung ist richtig; es wäre ja blöd wenn man dem Crawler eine
URL gibt und der dann sagt \"mimimi, ich weiss es besser und weigere
mich erst mal das überhaupt zu laden\". Deswegen wir die übergebene URL
schon mal prinzipiell geladen.\
\

> <div>
>
> hotel24 hat geschrieben:\
> Gibt es eine Möglichkeit, diese Einstellung irgendwo zu ändern?\
>
> </div>

\
nein. Du rufst das ja explizit auf weil du das willst. Wenn du eine
7-Tage-Regel für die übergebenen URLs willst, darfst du die selber erst
nach sieben Tagen wirder dort als Crawl start einstellen.\
\

> <div>
>
> hotel24 hat geschrieben:\
> Abgesehen davon, was passiert, wenn eine erfolgreich aufgenommene
> Seite im Zuge des nächsten Durchlaufes nicht mehr vorhanden ist und zb
> einen 410er zurückliefert. Werden die ursprünglich indizierten Inhalte
> dann gelöscht?\
>
> </div>

\
Ja und Nein. Dafür gibt es die beiden Regeln \"Document Deletion\" und
\"Document Double-Check\". Bei \"Document Deletion\" wird einfach alles,
was du vor X tagen (das stellst du ein) erfasst hast gelöscht. Bei
\"Document Double-Check\" wird alles was zum neuen Ladezeitpunkt X Tage
alt ist als nicht-Double erkannt und neu geladen. Was du brauchst, ist
der erste Punkt (\"Document Deletion\") um alle Dokumente, die nicht
mehr existieren vor dem Crawl zu löschen. Falls sie dann noch
existieren, werden sie neu erfasst.\
\

> <div>
>
> hotel24 hat geschrieben:\
> Gibt es evtl. die Möglichkeit, dass der ursprünglich indizierte Inhalt
> bestehen bleibt und die Seite erst dann aus dem Index fällt, wenn
> diese zB 3 Mal hintereinander eine Fehlerseite zurückliefert?\
>
> </div>

\
Nein, aber das wäre ja auch nur sinnvoll wenn die Seite eine merkwürdige
Verfügbarkeit hätte. Der Fall \"Document Deletion\" mit einem
anständigen Zeitintervall (z.B. 1 Monat) wäre hier was angemessenes.\
\

> <div>
>
> hotel24 hat geschrieben:\
> PS: Ein kleiner Punkt ist mir nebenbei aufgefallen. Wenn man \"Use
> filter\" auswählt, dann deaktivieren sich die Checkboxen \"Delete
> sub-path\" und \"Delete only old\". Erst wenn man mit der Maus ins
> Feld \"Use filter\" klickt, werden die Checkboxen wieder aktiv.\
>
> </div>

\
ja hier stimmt noch was nicht. Es hat damit zu tun dass ein Deletion bei
Sub-Path nicht sinnvoll ist, wenn es keinen Sub-Path gibt. Aber die
Logik ist hier noch nicht ganz vollständig, muss man überlegen.

Statistik: Verfasst von
[Orbiter](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=2)
--- Do Feb 07, 2013 9:34 pm

------------------------------------------------------------------------
