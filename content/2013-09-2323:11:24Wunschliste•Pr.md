Wunschliste • Präprozessierung mittels XSLT
===========================================

Date: 2013-09-23 23:11:24

Bei der Frage, wie sich eventuell die Inhaltserschließung diverser
Websites verbessern lässt, würde ich gerne einen Vorschlag zur
Diskussion stellen, den ich bereits in einem anderen Projekt erfolgreich
anwenden konnte.\
\
Grundlage ist die Annahme, dass YaCy jeweils dedizierte Webseiten
crawlt, also ein Crawl eines bestimmten Webangebotes (oder Teils davon)
vorgenommen wird. Weiter trifft zu, dass Weangebote in der Mehrheit
Template-basiert sind, der Inhalt also einer definierbaren Struktur
folgt. So lässt sich formal bestimmen, welche Fragmente der Seite
inhaltstragend sind und welche für die Indexierung uninteressant sind,
oder aber für den Crawler relevant (navigationale Elemente).\
Mein Vorschlag wäre, zwischen Crawler und Indexer die Möglichkeit
einzuführen, die erfassten Seiten mittels XSL-Skripten manipulieren zu
können. Gerade für dedoizierte Crawls könnte ein Transformationsskript
etwa die Seiten nach ihrem Inhalt \"selektieren\", bzw. auf die
relevanten Teile \"stutzen\". So könnte etwa eine News-Seite auf den
jeweiligen Artikel reduziert werden (alle umgebenden Elemente werden
entfernt) und relevante Links in einer Liste gesammelt angefügt werden.
Der Indexer würde dann nur den relevanten Text indizieren, der Crawler
hingegen den relevanten Links aus der entsprechenden Liste folgen
können. Bei reinen Portal-Seiten könnte der Inhalt hingegen gänzlich
verschwinden und nur die Links auf relevante Teile wiedergegeben
werden.\
Eventuell lässt sich mittels der Skripten auch eine \"kleine Heuristik\"
bauen, die dabei helfen kann relevante Seiten/Elemente zu selektieren.
Ich denke, dass dies besonders für spezialisierte Peers interessant sein
könnte.\
Nachteilig würde sich das ganze sicher auf die Geschwindigkeit des
Crawlers auswirken, sodass man abwägen/abschätzen müsste, ob sich
tatsächlich Vorteile ergeben.\
\
Ein anderer Vorteil, der sich quasi nebenbei noch ergeben würde: im
Rahmen der Transformation kann das Prinzip der Blacklists noch feiner
definiert werden, da im Rahmen der Transformation das Wissen über
Struktur und Inhalt der Seite vorliegt. Ein solches Skript könnte
folglich weitere Links aufgrund der \"Umgebung\" ausschließen oder gar
hinzufügen, bzw. modifizieren.

Statistik: Verfasst von
[surfvive](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=8791)
--- Mo Sep 23, 2013 10:11 pm

------------------------------------------------------------------------
