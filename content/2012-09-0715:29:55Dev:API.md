Dev:API
=======

Date: 2012-09-07 15:29:55

[Understanding YaCy crawl profiles: ]{.autocomment} update

← Nächstältere Version

Version vom 7. September 2012, 13:29 Uhr

Zeile 612:

Zeile 612:

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'crawlingMode\'\'\' =  

</div>

 

<div>

\|\'\'\'crawlingMode\'\'\' =  

</div>

−

<div>

\|

</div>

\+

<div>

\| [The crawler can be started with different modes.]{.underline}

</div>

−

<div>

\*url:

</div>

\+

<div>

\*url: [start from a given url which is the root of a crawl tree. The
url is given in \'crawlingURL\'. If this url is a http-link, then the
Crawl will subsequently load all linked documents from http until a
given depth is reached. If this url is a smb- or ftp-link, then the
given resource will be listed completely using a special listing
process.]{.underline}

</div>

−

<div>

\*sitemap:

</div>

\+

<div>

\*sitemap: [use a sitemap to retrieve all files that are listed in the
sitemap. The sitemap-URL is given in \'sitemapURL\']{.underline}

</div>

−

<div>

\*sitelist:

</div>

\+

<div>

\*sitelist: [use a list of crawl start URLs. This is like starting with
one url, but using several of them. The list of urls is retrieved by
loading the url given in \'crawlingURL\'. Each of the urls in that file
is then used to start it\'s individual crawl. This makes sense if the
\'range\' attribute contains the value \'domain\' or \'subpath\' which
creates an individual must-match pattern for each of the urls in the
sitelist.]{.underline}

</div>

−

<div>

\*file:

</div>

\+

<div>

\*file: [use a file in the local file system to provide a start
document. The crawl will then start like with a root url but the file
itself will not be placed to the index, only the documents which are
linked in that document.]{.underline}

</div>

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'crawlingURL\'\'\' =  

</div>

 

<div>

\|\'\'\'crawlingURL\'\'\' =  

</div>

Zeile 622:

Zeile 622:

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'sitemapURL\'\'\' =

</div>

 

<div>

\|\'\'\'sitemapURL\'\'\' =

</div>

−

<div>

\|

</div>

\+

<div>

\|[A url that is typically linked within a robots.txt file. The
sitemapURL must point to a resource which is formed as described in
http://www.sitemaps.org]{.underline}

</div>

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'crawlingFile\'\'\' =

</div>

 

<div>

\|\'\'\'crawlingFile\'\'\' =

</div>

−

<div>

\|

</div>

\+

<div>

\|[A path to the local file system.]{.underline}

</div>

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'crawlingDepth\'\'\' =

</div>

 

<div>

\|\'\'\'crawlingDepth\'\'\' =

</div>

−

<div>

\|This defines how often the Crawler will follow links embedded in
websites.~~\<br /\>~~

</div>

\+

<div>

\|This defines how often the Crawler will follow links embedded in
websites.

</div>

 

<div>

A minimum of 0 is recommended and means that the page set as crawling
URL, sitemap orfile will be added to the index, but no linked content is
indexed. 2-4 is good for normal indexing. Be careful with the depth,
consider a branching factor of average 20; A prefetch-depth of 8 would
index 25.600.000.000 pages, maybe this is the whole WWW.

</div>

 

<div>

A minimum of 0 is recommended and means that the page set as crawling
URL, sitemap orfile will be added to the index, but no linked content is
indexed. 2-4 is good for normal indexing. Be careful with the depth,
consider a branching factor of average 20; A prefetch-depth of 8 would
index 25.600.000.000 pages, maybe this is the whole WWW.

</div>

 

<div>

\|-

</div>

 

<div>

\|-

</div>

Zeile 647:

Zeile 647:

 

<div>

\|-

</div>

 

<div>

\|-

</div>

 

<div>

\|\'\'\'crawlingIfOlderUnit\'\'\' =  

</div>

 

<div>

\|\'\'\'crawlingIfOlderUnit\'\'\' =  

</div>

−

<div>

~~\|~~

</div>

 

−

<div>

~~\|-~~

</div>

 

−

<div>

~~\|\'\'\'crawlingDomFilterCheck\'\'\' =~~

</div>

 

−

<div>

~~\|This option will automatically create a domain-filter which limits
the crawl on domains the crawler will find on the given depth. You can
use this option i.e. to crawl a page with bookmarks while restricting
the crawl on only those domains that appear on the bookmark-page. The
adequate depth for this example would be 1. The default value 0 gives no
restrictions.~~

</div>

 

−

<div>

~~\|-~~

</div>

 

−

<div>

~~\|\'\'\'crawlingDomFilterDepth\'\'\' =~~

</div>

 

 

<div>

\|

</div>

 

<div>

\|

</div>

 

<div>

\|-

</div>

 

<div>

\|-

</div>
