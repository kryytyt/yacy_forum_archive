Hilfe für Einsteiger und Anwender • Re: Crawl auf Startseiten beschränken
=========================================================================

Date: 2013-01-23 00:35:40

Soweit ist das eigentlich das richtige Vorgehen und ich würde sagen das
sollte genau so klappen \...\
Ich habe das ebenfalls gerade ausprobiert und ich habe auf der Seite für
den [Crawl Start
(Experte)](http://localhost:8090/CrawlStartExpert_p.html){.postlink} die
URLs der Domänen einfach in das Eingabefeld kopiert mit einer Domäne pro
Zeile.\
Bei 20000 Links ist das natürlich besser aus einer Datei heraus - von
daher sollte auch das kein Problem sein und das Format der Datei habe
ich genauso benutzt und einfach pro Zeile eine URL.\
Damit die robots.txt nicht indexiert wird habe ich diese zusätzlich im
Must-Not-Match Filter: angegeben.\
\
Hier z.B. das Ergebnis für <http://www.google.de> im Host Browser bei
dem nur das Hauptdokument indexiert wurde:\
YaCy\_google.png\
\
Ist zwar nur 50% aber hier die Antworten im Überblick:\
1) Was muss ich einstellen, damit exakt die 20.000 in der Datei
angegebenen Urls indiziert werden und nichts anderes sonst?\
1.A) Bei mir wurde jeweils genau die Startseite indexiert und mit dem
Ausschluss der robots.txt sollte auch diese Datei nicht auftauchen\
\
2) Kann man irgendwo sehen, was bei jeder Url gefunden wurde? (also zB
\"200 ok\", oder \"404 not found\", \"301 moved\", etc.)\
2.A) Ja, aktuell sehr übersichtlich im [Host
Browser](http://localhost:8090/HostBrowser.html){.postlink}\
\
3) Wie viele Urls können maximal in der Datei angegeben werden?\
3.?) Ich denke da gibt es kein Limit das ich direkt angeben könnte. Wenn
Deine Maschine bzw. der Arbeitsspeicher der YaCy Instanz ausreicht kann
man sicher noch weit mehr als 20000 Einträge in die Liste packen.\
\
4) Besteht die Möglichkeit, einen erneuten Crawl auf nur zB alle \"404
not found\"-Seiten zu beschränken und zB die \"200 ok\"-Seiten
unverändert im Index zu belassen?\
4.?) Im Host Browser kann man sich die Liste aller fehlgeschlagenen
Crawls / Host zwar anzeigen lassen - aber eine einfache Möglichkeit
genau diese \"404 not found\"-Seiten erneut crawlen zu lassen wüsste ich
nicht. Vielleicht kann ja jemand anderes weiterhelfen.

Statistik: Verfasst von
[Copro](http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&u=174)
--- Mi Jan 23, 2013 12:35 am

------------------------------------------------------------------------
