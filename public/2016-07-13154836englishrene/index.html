<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.37.1" />
  <title> &middot; Yacy Old Forum Archiv</title>
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">
  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->
  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->
  <link rel="stylesheet" href="/css/blackburn.css">
  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 
  
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
  
  
</head>
<body>
<div id="layout">
  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">
  
  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
    </ul>
  </div>
  <div class="pure-menu social">
  <ul class="pure-menu-list">
    
    
    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/http://twitter.com/yacy_search" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/yacy" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>
  <div>
  <div class="small-print">
    <small></small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>
</div>
  <div id="main">
<div class="header">
  <h1></h1>
  <h2></h2>
</div>
<div class="content">
  
<h1 id="english-re-new-page-failed-reason-exist-test-failed-error-execu">English â€¢ Re: new page =failed. Reason: exist-test failed: Error execu</h1>
<p>Date: 2016-07-13 15:48:36</p>
<p>HI + thanks for replying.<br />
I, too, just crawled that page and got this:<br />
Crawling of
\&ldquo;<a href="http://www.theguardian.com/politics/2016/jul/11/who-will-be-in-theresa-mays-cabinet-government">http://www.theguardian.com/politics/2016/jul/11/who-will-be-in-theresa-mays-cabinet-government</a>
\&rdquo; failed. Reason: exist-test failed: Error executing query/<br />
<br />
The server in a Win vm is running as stand alone Robinson mode - public
peer<br />
(Microsoft Windows 10 instance in Azure cloud computing environment)<br />
The idea is to later convert several such Robinson servers to a
dedicated private group with full DHT+P2P in a privte group of servers
for searching a special topic<br />
Network definition = defaults/yacy.network.allip.unit<br />
<br />
re: used in /CrawlStartExpert.html<br />
generic, excepting these:<br />
Crawling Depth = 0</p>
<blockquote>
<div>
\
\...Use Special User Agent and robot identification\
</div>
</blockquote>
<p><br />
Use Special User Agent and robot identification = Random browser or
\&lsquo;greedy\&rsquo; mode<br />
<br />
As listed below...<br />
Crawling Depth 1 + also all linked non-parsable documents [selected]<br />
Unlimited crawl depth for URLs matching with [not selected]<br />
Maximum Pages per Domain Use: [not selected] Page-Count: [not
selected] 10000<br />
misc. Constraints Accept URLs with query-part (\&lsquo;?\&lsquo;): [selected]<br />
Obey html-robots-noindex: [selected]<br />
Obey html-robots-nofollow: [not selected]<br />
Load Filter on URLs must-match<br />
Restrict to start domain(s) [selected]<br />
Restrict to sub-path(s) [not selected]<br />
Use filter.* [not selected] (must not be empty)<br />
must-not-match<br />
Load Filter on IPs .* [not selected] must-match(must not be empty)<br />
must-not-match [not selected]<br />
Must-Match List for Country Codes info no country code restriction<br />
Use filter [not selected]<br />
AD,AL,AT,BA,BE,BG,BY,CH,CY,CZ,DE,DK,EE,ES,FI,FO,FR,GG,GI,GR,HR,HU,IE,IM,IS,IT,JE,LI,LT,LU,LV,MC,MD,MK,MT,NL,NO,PL,PT,RO,RU,SE,SI,SJ,SK,SM,TR,UA,UK,VA,YU<br />
Document Filter [not selected]<br />
These are limitations on index feeder. The filters will be applied after
a web page was loaded.<br />
<br />
Filter on URLsinfo<br />
must-match .* [not selected] (must not be empty)<br />
must-not-match[not selected]<br />
Filter on Content of Document [not selected]<br />
(all visible text, including camel-case-tokenized url and title)<br />
must-match .* [not selected](must not be empty)<br />
must-not-match[not selected]<br />
Clean-Up before Crawl Start No Deletion Do not delete any document
before the crawl is started.<br />
Delete sub-path [not selected] For each host in the start url list,
delete all documents (in the given subpath) from that host.<br />
Delete only old [not selected] Treat documents that are loaded ago as
stale and delete them before the crawl is started.<br />
Double-Check Rules<br />
No Doubles [selected] Never load any page that is already known. Only
the start-url may be loaded again.<br />
Re-load [not selected] Treat documents that are loaded ago as stale
and load them again. If they are younger, they are ignored.<br />
Document Cache<br />
Store to Web Cache [selected]<br />
Policy for usage of Web Cache<br />
no cache [not selected] if fresh [selected] if exist [not
selected] cache only [not selected]<br />
Robot Behaviour<br />
Use Special User Agent and robot identification [Random Browser]<br />
Snapshot Creation<br />
Max Depth for Snapshots -1<br />
Multiple Snapshot Versions [selected&gt;] replace old snapshots with new
one [not selected] add new versions for each crawl must-not-match
filter for snapshot generation<br />
Index Attributes<br />
Indexing<br />
index text: {selected] index media: [selected] Add Crawl result to
collection(s) user<br />
Time Zone Offset -120<br />
<br />
---<br />
How and where did you get YaCy 1.<sup>91</sup>&frasl;<sub>9013</sub> please? I\&rsquo;d like to update
everything to latest ver., please, ASAP. Thanks!<br />
I cannot find it and the win version: 1.<sup>90</sup>&frasl;<sub>9000</sub> update does not show it
as available, and from here it\&rsquo;s somehow not in Google search.<br />
<br />
Many thanks for your patient help!</p>
<p>Statistik: Verfasst von
<a href="http://forum.yacy-websuche.de/memberlist.php?mode=viewprofile&amp;u=9463">xioc752</a>
&mdash; Mi Jul 13, 2016 2:48 pm</p>
<hr />
</div>
</div>
</div>
<script src="js/ui.js"></script>
</body>
</html>
